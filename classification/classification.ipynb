{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3fe6a0f",
   "metadata": {},
   "source": [
    "# Classification de tweets avec un LLM (Ollama + Mistral)\n",
    "\n",
    "Dans ce notebook, on va :\n",
    "\n",
    "1. Charger un fichier CSV de tweets nettoy√©s.\n",
    "2. Pr√©parer un prompt avec quelques exemples (few-shot).\n",
    "3. Appeler un mod√®le LLM local (Ollama, mod√®le *mistral*) pour classer chaque tweet.\n",
    "4. R√©cup√©rer la r√©ponse du mod√®le en JSON et la parser proprement.\n",
    "5. Normaliser les r√©sultats (topics, sentiment, incident, etc.).\n",
    "6. Fusionner les pr√©dictions avec le CSV d‚Äôorigine et sauvegarder le r√©sultat.\n",
    "\n",
    "Ce notebook reprend la logique du script Python, mais de mani√®re plus p√©dagogique et √©tape par √©tape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf95857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports des biblioth√®ques n√©cessaires\n",
    "\n",
    "import requests          # pour appeler l'API HTTP d'Ollama\n",
    "import pandas as pd      # pour lire/√©crire les CSV\n",
    "import orjson           # pour parser le JSON renvoy√© par le mod√®le\n",
    "import time             # pour faire des pauses entre les retries\n",
    "import sys              # pour quitter proprement (utile si fichier manquant)\n",
    "from pathlib import Path # pour travailler proprement avec les chemins de fichiers\n",
    "\n",
    "# ====== CONFIG GENERALE ======\n",
    "\n",
    "MODEL = \"mistral\"                                 # Nom du mod√®le Ollama\n",
    "SOURCE_CSV = \"../data/free_tweet_export.cleaned.llm.csv\"  # Fichier des tweets nettoy√©s\n",
    "OUTPUT_CSV = \"../data/free_tweet_classified_clean.csv\"    # Fichier de sortie avec les pr√©dictions\n",
    "TEXT_COL = \"text_clean_llm\"                       # Nom de la colonne texte dans le CSV\n",
    "\n",
    "# Param√®tres pour √©ventuellement limiter le nombre de lignes (pour debug) :\n",
    "SAMPLE_N = None        # par ex: 200 pour tester uniquement sur 200 lignes\n",
    "SAVE_SAMPLE_AS = None  # ex: \"echantillon.csv\" pour sauvegarder l'√©chantillon\n",
    "\n",
    "# Param√®tres d'appel √† l'API\n",
    "API_URL = \"http://localhost:11434/api/chat\"\n",
    "BATCH_SIZE = 20        # nombre de tweets √† traiter par batch\n",
    "HTTP_TIMEOUT = 300     # timeout HTTP en secondes\n",
    "MAX_RETRIES = 3        # nombre de tentatives en cas d'erreur\n",
    "BACKOFFS = [1, 3, 7]   # temps d'attente entre tentatives\n",
    "\n",
    "# Session HTTP r√©utilis√©e pour optimiser les appels r√©seau\n",
    "SESSION = requests.Session()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb9d11f",
   "metadata": {},
   "source": [
    "## Chargement des donn√©es\n",
    "\n",
    "Dans cette partie, on va :\n",
    "\n",
    "- V√©rifier que le fichier CSV source existe.\n",
    "- Charger le CSV dans un DataFrame `df_full`.\n",
    "- V√©rifier que la colonne texte choisie (`TEXT_COL`) est bien pr√©sente.\n",
    "- (Optionnel) Prendre seulement un √©chantillon des donn√©es si `SAMPLE_N` est d√©fini.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50a0f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rification de l'existence du fichier\n",
    "if not Path(SOURCE_CSV).exists():\n",
    "    print(f\"Fichier introuvable: {SOURCE_CSV}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Lecture du fichier CSV complet\n",
    "df_full = pd.read_csv(SOURCE_CSV, engine=\"python\")\n",
    "print(f\"Colonnes trouv√©es dans le CSV : {list(df_full.columns)}\")\n",
    "\n",
    "# V√©rification de la pr√©sence de la colonne texte\n",
    "if TEXT_COL not in df_full.columns:\n",
    "    raise SystemExit(f\"Colonne {TEXT_COL} introuvable dans {SOURCE_CSV}\")\n",
    "\n",
    "# Gestion de l'√©chantillonnage\n",
    "if SAMPLE_N is not None:\n",
    "    df = df_full.head(int(SAMPLE_N)).copy()\n",
    "    if SAVE_SAMPLE_AS:\n",
    "        df.to_csv(SAVE_SAMPLE_AS, index=False, encoding=\"utf-8\")\n",
    "        print(f\"üíæ √âchantillon sauvegard√© dans {SAVE_SAMPLE_AS} ({len(df)} lignes)\")\n",
    "    else:\n",
    "        print(f\"üîé √âchantillon en m√©moire: {len(df)} lignes (non sauvegard√©)\")\n",
    "else:\n",
    "    df = df_full.copy()\n",
    "    print(f\"üîé Traitement de tout le fichier: {len(df)} lignes\")\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc375ae",
   "metadata": {},
   "source": [
    "## D√©finition du prompt et des exemples (few-shot)\n",
    "\n",
    "Ici, on pr√©pare :\n",
    "\n",
    "- Le *prompt syst√®me* (contexte global) lu depuis un fichier `prompt_tweets_free.txt`.\n",
    "- Un template de message utilisateur qui contient le tweet et un exemple de JSON attendu.\n",
    "- Une liste d'exemples `FEW_SHOTS` (tweet ‚Üí r√©ponse JSON) pour montrer au mod√®le le format voulu.\n",
    "- La variable `BASE_MESSAGES` qui contient :\n",
    "  - le message syst√®me\n",
    "  - les exemples few-shot\n",
    "\n",
    "Cette base sera r√©utilis√©e pour chaque appel au mod√®le.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438c91cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture du prompt syst√®me depuis un fichier texte\n",
    "with open(\"prompt_tweets_free.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    SYSTEM_PROMPT = f.read()\n",
    "\n",
    "# Template du message utilisateur (on y ins√©rera chaque tweet)\n",
    "USER_TEMPLATE = \"\"\"Tweet:\n",
    "{tweet}\n",
    "\n",
    "R√©ponds en JSON (une seule ligne), exemple:\n",
    "{{\"is_claim\":1,\"topics\":[\"fibre\"],\"sentiment\":\"neg\",\"urgence\":\"haute\",\"incident\":\"incident_reseau\",\"confidence\":0.82}}\"\"\"\n",
    "\n",
    "# Exemples few-shot (tweet -> JSON attendu)\n",
    "FEW_SHOTS = [ \n",
    "    (\"rt @free: d√©couvrez la nouvelle cha√Æne imearth en 4k !\",\n",
    "     '{\"is_claim\":0,\"topics\":[\"tv\"],\"sentiment\":\"neu\",\"urgence\":\"basse\",\"incident\":\"information\",\"confidence\":0.9}'),\n",
    "    (\"@free panne fibre √† cergy depuis 7h, impossible de bosser\",\n",
    "     '{\"is_claim\":1,\"topics\":[\"fibre\"],\"sentiment\":\"neg\",\"urgence\":\"haute\",\"incident\":\"incident_reseau\",\"confidence\":0.9}'),\n",
    "    (\"@freebox non mais vous r√©pondez trois jours apr√®s... super le service apr√®s vente\",\n",
    "     '{\"is_claim\":1,\"topics\":[\"autre\"],\"sentiment\":\"neg\",\"urgence\":\"moyenne\",\"incident\":\"processus_sav\",\"confidence\":0.85}')\n",
    "]\n",
    "\n",
    "# Pr√©paration de la base de messages commune : system + few-shot\n",
    "BASE_MESSAGES = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "for u_text, y_json in FEW_SHOTS:\n",
    "    BASE_MESSAGES.append({\"role\": \"user\", \"content\": u_text})\n",
    "    BASE_MESSAGES.append({\"role\": \"assistant\", \"content\": y_json})\n",
    "\n",
    "print(\"Prompt syst√®me et few-shots initialis√©s.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e3fd94",
   "metadata": {},
   "source": [
    "## Fonctions utilitaires\n",
    "\n",
    "Nous allons d√©finir trois fonctions importantes :\n",
    "\n",
    "1. `chat_ollama_batch(model, prompts)`  \n",
    "   ‚Üí envoie les prompts au mod√®le Ollama (un par un), avec gestion des erreurs et retries.\n",
    "\n",
    "2. `_norm_incident(x)`  \n",
    "   ‚Üí normalise la variable `incident` pour la ramener √† un petit set de valeurs autoris√©es\n",
    "   (ex. \"r√©seau\", \"sav\", \"support\" ‚Üí cat√©gories standard).\n",
    "\n",
    "3. `parse_json_line(s)`  \n",
    "   ‚Üí essaie de parser proprement la r√©ponse du LLM en JSON,\n",
    "   m√™me si la r√©ponse est entour√©e de ```json ... ``` ou contient du texte en plus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c60077a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_ollama_batch(model: str, prompts: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Appelle Ollama (API chat) de mani√®re s√©quentielle avec retries/backoff.\n",
    "    - `prompts` : liste de textes (un par tweet)\n",
    "    - Retour : liste de r√©ponses brutes (strings), une par prompt\n",
    "    \"\"\"\n",
    "    out: list[str] = []\n",
    "\n",
    "    for p in prompts:\n",
    "        # messages = system + few-shots + message utilisateur\n",
    "        messages = BASE_MESSAGES + [{\"role\": \"user\", \"content\": p}]\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"messages\": messages,\n",
    "            \"stream\": False,\n",
    "            \"options\": {\"temperature\": 0.1}\n",
    "        }\n",
    "\n",
    "        last_err = None\n",
    "        for attempt in range(1, MAX_RETRIES + 1):\n",
    "            try:\n",
    "                r = SESSION.post(API_URL, json=payload, timeout=HTTP_TIMEOUT)\n",
    "                r.raise_for_status()\n",
    "                data = r.json()\n",
    "                content = data.get(\"message\", {}).get(\"content\", \"\").strip()\n",
    "                if content:\n",
    "                    out.append(content)\n",
    "                    break\n",
    "                else:\n",
    "                    last_err = \"empty_content\"\n",
    "                    raise RuntimeError(\"Empty content from model\")\n",
    "            except Exception as e:\n",
    "                last_err = str(e)\n",
    "                # si erreur, on attend un peu avant de retenter\n",
    "                if attempt < MAX_RETRIES:\n",
    "                    time.sleep(BACKOFFS[min(attempt-1, len(BACKOFFS)-1)])\n",
    "                else:\n",
    "                    # si toutes les tentatives √©chouent, on renvoie un JSON d'erreur minimal\n",
    "                    safe_err = last_err.replace('\"', \"'\") if last_err else \"unknown_error\"\n",
    "                    out.append(\n",
    "                        '{\"is_claim\":0,\"topics\":[\"autre\"],\"sentiment\":\"neu\",\"urgence\":\"basse\",\"incident\":\"autre\",\"confidence\":0.0,\"_error\":\"%s\"}'\n",
    "                        % safe_err\n",
    "                    )\n",
    "    return out\n",
    "\n",
    "\n",
    "def _norm_incident(x: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalise la valeur 'incident' vers un set de valeurs autoris√©es,\n",
    "    en g√©rant quelques alias courants.\n",
    "    \"\"\"\n",
    "    allowed = {\"facturation\", \"incident_reseau\", \"livraison\", \"information\", \"processus_sav\", \"autre\"}\n",
    "    s = str(x).strip().lower().replace(\" \", \"_\")\n",
    "\n",
    "    aliases = {\n",
    "        \"reseau\": \"incident_reseau\",\n",
    "        \"r√©seau\": \"incident_reseau\",\n",
    "        \"incident\": \"incident_reseau\",\n",
    "        \"sav\": \"processus_sav\",\n",
    "        \"service_apres_vente\": \"processus_sav\",\n",
    "        \"service_apr√®s_vente\": \"processus_sav\",\n",
    "        \"support\": \"processus_sav\",\n",
    "        \"facture\": \"facturation\",\n",
    "        \"info\": \"information\",\n",
    "        \"livraisons\": \"livraison\"\n",
    "    }\n",
    "\n",
    "    s = aliases.get(s, s)\n",
    "    return s if s in allowed else \"autre\"\n",
    "\n",
    "\n",
    "def parse_json_line(s: str) -> dict:\n",
    "    \"\"\"\n",
    "    Parse robuste de la ligne JSON renvoy√©e par le LLM.\n",
    "    - G√®re les blocs ```json ... ```\n",
    "    - Essaie d'extraire la partie entre { et }\n",
    "    - Renvoie un dict par d√©faut si tout √©choue\n",
    "    \"\"\"\n",
    "    s = str(s).strip()\n",
    "\n",
    "    # Gestion des blocs ```json ... ```\n",
    "    if s.startswith(\"```\"):\n",
    "        lines = s.splitlines()\n",
    "        if len(lines) >= 3 and lines[0].startswith(\"```\") and lines[-1].startswith(\"```\"):\n",
    "            s = \"\\n\".join(lines[1:-1]).strip()\n",
    "        else:\n",
    "            # fallback : on enl√®ve les backticks et un √©ventuel 'json'\n",
    "            s = s.strip(\"`\").strip()\n",
    "            if s.lower().startswith(\"json\"):\n",
    "                s = s[4:].strip()\n",
    "\n",
    "    # Premi√®re tentative : parse direct\n",
    "    try:\n",
    "        return orjson.loads(s)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Deuxi√®me tentative : on isole le premier { et le dernier }\n",
    "    try:\n",
    "        start = s.find(\"{\")\n",
    "        end = s.rfind(\"}\")\n",
    "        if start != -1 and end != -1 and end > start:\n",
    "            return orjson.loads(s[start:end+1])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback : objet par d√©faut\n",
    "    return {\n",
    "        \"is_claim\": 0,\n",
    "        \"topics\": [\"autre\"],\n",
    "        \"sentiment\": \"neu\",\n",
    "        \"urgence\": \"basse\",\n",
    "        \"incident\": \"autre\",\n",
    "        \"confidence\": 0.0,\n",
    "        \"_parse_error\": True\n",
    "    }\n",
    "\n",
    "print(\"Fonctions utilitaires pr√™tes.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
